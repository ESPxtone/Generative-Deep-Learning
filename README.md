# Generative-Deep-Learning

* mlp+cnn - распознование изображений с помощью многослойного перцептрона + сверточной нейронной сети.
* autoencoder - обучение автокодировщика на датасете fashion_mnist, размерность скрытого пространства = 2.
  Автокодировщик: кодировщик сжимает исходное изображение в пространство меньшей размерности, декодировщик восстанавливает сжатый вектор в исходное изображение.
  При выборе случайноого вектора из скрытого пространства происходит генерация новного изображения.
  Проблема автокодировщика в том, что скрытое пространство НЕ непрерывно, что приводит к не лучшей реконструкции ихображений, особенно при большой размерности скрытого пространства.
* vae_fashion - вариационный автокодировщик, датасет fashion_mnist. размерность скрытого простарнства 2.
  Отличие вариационного автокодировщика в том, что исходное изображение сжимается не в точку, а в многомерное нормальное распределение вокруг точки в скрытом пространстве.
  Отображение в нормальное распределение повышает непрерывность пространства и декодировщик декодирует соседине точки в более похожие изображения.
  Также в функцию потерь добавляется расстояние Кульбака-Лейблера(Kullback-Leibler divergence), которое штрафует модель если распределение отличается от нормального.
* vae_faces - вариационный автокодировщик, датасет celeba. размерность скрытого простарнства 200.
  Чем больше скрытое пространство тем больше параметров может выделить кодировщик.
  Также здесь показаны возможности преобразования изображений с помощью операций над векторами в скрытом пространстве.
* dcgan - Convolutional Generative Adversarial Network (DCGAN). Светрочная генеративно-состязательная модель: состоит из дискриминатора и генератора.
  Генератор создаёт изображение из случайного вектора (шума) также как декодер в автокодировщике, дискриминатор по сути обычный классификатор,
  который учится отличать сгенерированные и настоящие изображения. Оценки дискриминатора являются функцией потерь для генератора(бинарная кросс-энтропия), таким образом модели постоянно
  улучшают друг друга, поочередно обновляя веса. Проблема GAN в том, что одна из моделей может начать доминировть над другой, что приведёт к коллапсу функции потерь. Если начинает доминировать дискриминатор можно:
  увеличить dropout, уменьшить скорость обучения, уменьшить кол-во сверточных слоев, добавить шум в обучение дискриминатора. Если доминирует генератор - услиливаем дискриминатор, делая обратные шаги.
