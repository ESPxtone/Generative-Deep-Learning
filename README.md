# Generative-Deep-Learning

* mlp+cnn - распознование изображений с помощью многослойного перцептрона + сверточной нейронной сети.
* autoencoder - обучение автокодировщика на датасете fashion_mnist, размерность скрытого пространства = 2.
  Автокодировщик: кодировщик сжимает исходное изображение в пространство меньшей размерности, декодировщик восстанавливает сжатый вектор в исходное изображение.
  При выборе случайноого вектора из скрытого пространства происходит генерация новного изображения.
  Проблема автокодировщика в том, что скрытое пространство НЕ непрерывно, что приводит к не лучшей реконструкции ихображений, особенно при большой размерности скрытого пространства.
* vae_fashion - вариационный автокодировщик, датасет fashion_mnist. размерность скрытого простарнства 2.
  Отличие вариационного автокодировщика в том, что исходное изображение сжимается не в точку, а в многомерное нормальное распределение вокруг точки в скрытом пространстве.
  Отображение в нормальное распределение повышает непрерывность пространства и декодировщик декодирует соседине точки в более похожие изображения.
  Также в функцию потерь добавляется расстояние Кульбака-Лейблера(Kullback-Leibler divergence), которое штрафует модель если распределение отличается от нормального.
* vae_faces - вариационный автокодировщик, датасет celeba. размерность скрытого простарнства 200.
  Чем больше скрытое пространство тем больше параметров может выделить кодировщик.
  Также здесь показаны возможности преобразования изображений с помощью операций над векторами в скрытом пространстве.
* dcgan - Convolutional Generative Adversarial Network (DCGAN). Светрочная генеративно-состязательная модель: состоит из дискриминатора и генератора.
  Генератор создаёт изображение из случайного вектора (шума) также как декодер в автокодировщике, дискриминатор по сути обычный классификатор,
  который учится отличать сгенерированные и настоящие изображения. Оценки дискриминатора являются функцией потерь для генератора(бинарная кросс-энтропия), таким образом модели постоянно
  улучшают друг друга, поочередно обновляя веса. Проблема GAN в том, что одна из моделей может начать доминировть над другой, что приведёт к коллапсу функции потерь. Если начинает доминировать дискриминатор можно:
  увеличить dropout, уменьшить скорость обучения, уменьшить кол-во сверточных слоев, добавить шум в обучение дискриминатора. Если доминирует генератор - услиливаем дискриминатор, делая обратные шаги.
  Проблема в том, что нету прямой корреляции между потерями дискриминатора и генератора (это решается WGAN).
* wgan-gp - Wasserstein GAN  with Gradient Penalty. GAN с использованием функции потерь Вассерштайна вместо бинарной кросс-энтропии, она использует метки -1,1 вместо 1,0 и вместо сигмойды на выходите дискриминатора имеет значения в бесконечном диапазоне. В такой реализации дискриминатор обычно называют критиком (потому что она даёт не вероятность, а оценку). Критик WGAN пытается максимизировать разницу между оценками реальных и сгенерированных изображений, при этом реальные изображения получают оценки выше. Генератор WGAN пытается создать изображения, которые будут получать высокие оценки критика. Так как критик выдаёт оценки в бесконечном диапазоне стоит задать ему определенные ограничения (Ограничение Липшица) - критик должен быть одномерной липщицевой непрерывной функией, по сути , мы ограничиваем скорость с которой могут меняться оценки для двух разных изображений (градиент <= 1). Можно ввести ограничения на веса критика, но это плохой вариант, вместо этого в функцию потерь добавляется штраф за градиент - измеряется квадрат разности между нормой градиента прогнозов для входных изображений и 1. Так как градиент сложно вычислить повсюду вместо этого мы его вычисляем только в некоторых точках (интерполированные изображения). Все эти дополнения повышают стабильность обучения GAN и упрощают оценки функции потерь (она все время сходится). Также важно НЕ использовать пакетную нормализацию в WGAN (она снижает эффективность штрафа за градиент).
* cgan - условная GAN (conditional GAN). Критику и генерату передаётся дополнительная информация с метками (параметрами изображения). На входе генератора она добавляется в точку скрытого пространства как вектор прямого кодирования.
  На вход критика информация о метк едобавляется как дополнительный канал к изображению RGB. В итоге генератор должен не просто создать 'правдоподобное' изображение, но и чтобы оно соответствовало метке.
  В итоге мы можем управлять выводом CGAN, передавая на вход генератора конкретные метки в формате прямого кодирования
* lstm - модель авторегрессии - генерация как последовательный процесс, прогноз опирается на предыдущие значения в последовательности. LSTM (Long Short-Term Memory) - разновидность рекуррентной сети (RNN), содержашею рекуррентный слой
  сопсобный обрабатывать последовательные данные. Обычная рекурретная ячейка(слой) обновляет скрытое состояние (вектор длинной = кол-во узлов) на основе предыдущего временного шага и текущего временного шага.
  Слой LSTM - возращает новое скрытое состояние, опираясь на предыдущее скрытое состояние и векторное представление текущего слова, ячейка поддерживает состояние которое можно рассматривать как ее внутреннее убеждение в том, каково текущее 
  состояние последовательности.
* pixelcnn - авторегрессионная модель для генерации изображений. Главными элементами являются - маскировачный сверточный слой и остаточные блоки. Маскировачный сверточный слой - сверточный слой, который использует маску для скрытия пикселов
  не предшествующих рассматриваемому (такой подход позволяет рассматривать изображение как последовательность пикселов). Маски бывают двух типов А - маскирует значение центрального пиксела, B - не маскирует центральный пиксел.
  Остаточные блоки - набор слоев, которые лобовляют выходные данные к входным перед передачей в остальную часть сети. Так входные данные могут передаваться по короткому пути на выход без прохождения через промежуточные слои (skip connection).
  Сусть приема в том, что если оптимальное преобразование - сохранить входные данные неизменными, это можно сделать просто обнулив веса промежуточных слоев. Минус такой модели в низкой скорости работы, т.к. необходимо получать прогноз для 
  каждого пиксела по отдельности.
* pixelcnn_md - выходные данные в этой модели представляются как смесь распределений (с 256 дискретными значениями). Смесь распределений - это просто несколько вероятностных распределений, выбор значения из которых просиходит с помощью 
 многоуровнего дискретного распределения. Сначала выбирается распределение, а потом значение из него. Так для 3 распределений нужно всего 8 параметров - два для выбора распределения и средние значения с дисперсиями для трех распределений, 
 вместо 255 параметров по всему диапазону значений пикселов. Такая модель уже реализована в библиотеке tensorflow_probability, достаточно лишь задать параметры. Модель обучается как и раньше, но на входе целые значения пикселов от 0 до 255.
* realnvp - модель нормализующего потока. Это обратимая функция, которая позволяет напрямую моедлировать плотность распределения данных посредством замены переменных. Для замены необходимо 
вычислить определитель якобиана. Для простоты вычислений ReavNVP ограничивает сеть так, чтобы она была обратимой и имела просто вычисляемый определитель якобиана. Это осуществляется за счет комбинации слоев связи, создающих на каждом этапе коэффициенты масштабирования и переноса. Слой связи маскирует данные так, чтобы якобиан имел нижнетреугольную форму (лего вычислялся). Полная видимость данных достигается инвертированием масок в каждом слое. Итоговая модель может пропускать данные в прямом направлении (в нормальное распределение например) и обратном из скрытого пространства в новые сгенирированные данные.
